{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection and Keyword Extraction in Academic Papers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaMulticore\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keybert import KeyBERT\n",
    "import pickle\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "nltk.download('stopwords')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "DEBUG = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df = pd.read_csv('data/publications-converted.csv', quotechar=\"'\", delimiter=',', quoting=2)\n",
    "authors_df = pd.read_csv('data/authors_all.csv')\n",
    "users_df = pd.read_csv('data/users_all.csv')\n",
    "users_validated_df = pd.read_csv('data/users_validatAcceptat.csv')\n",
    "\n",
    "publications_df = publications_df.drop_duplicates()\n",
    "authors_df = authors_df.drop_duplicates()\n",
    "publications_df = publications_df.drop(columns=['no_coauthors', 'publication_type', 'no_pages', 'd_oi', \n",
    "                                                'category', 'file_link', 'external_link', 'publisher', 'w_os',\n",
    "                                                'jhi_type', 'cross_ref_validation', 'publication_date',\n",
    "                                                'file_link_shown', 'citations_number', 'metadata', 'internal_link',\n",
    "                                                'keywords_valid', 'photo_link', 'mapped_to_id', 'local_keywords',\n",
    "                                                'keywords_isi'])\n",
    "publications_df = publications_df.dropna(subset=['title', 'abstract_text', 'keywords', 'authors'])\n",
    "langs = publications_df['abstract_lang'].unique()\n",
    "print(f'Starting languages: {langs}')\n",
    "# Only one publication in 'UNKNOWN' language and it is in english\n",
    "publications_df.loc[publications_df['abstract_lang'] == 'UNKNOWN', 'abstract_lang'] = 'en'\n",
    "# Drop publications with no abstract language\n",
    "publications_df = publications_df.dropna(subset=['abstract_lang'])\n",
    "langs = publications_df['abstract_lang'].unique()\n",
    "print(f'Used languages: {langs}')\n",
    "\n",
    "# Group publications by language\n",
    "publications_df = publications_df.groupby('abstract_lang')\n",
    "\n",
    "publications_all_en = publications_df.get_group('en')\n",
    "publications_all_ro = publications_df.get_group('ro')\n",
    "\n",
    "publications_all_en = publications_all_en.drop(columns=['abstract_lang'])\n",
    "publications_all_ro = publications_all_ro.drop(columns=['abstract_lang'])\n",
    "\n",
    "print(f'Number of publications in english: {len(publications_all_en)}')\n",
    "print(f'Number of publications in romanian: {len(publications_all_ro)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various methods of fetching and filtering publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publications_by_user_name(publications, authors, users, first_name, last_name):\n",
    "    if len(first_name) == 0 or len(last_name) == 0:\n",
    "        print('First name or last name is empty, returning full publications set')\n",
    "        return pd.DataFrame()\n",
    "    first_name = first_name.lower()\n",
    "    last_name = last_name.lower()\n",
    "    user = users.loc[(users['first_name'].str.lower().str.contains(first_name)) & (users['last_name'].str.lower().str.contains(last_name))].values\n",
    "    if len(user) == 0:\n",
    "        print(f'No user found with first name: {first_name} and last name: {last_name}')\n",
    "        return pd.DataFrame()\n",
    "    user = user[0]\n",
    "    user_publications_ids = authors.loc[authors['user_id'] == user[0]]['publication_id'].values\n",
    "    if len(user_publications_ids) == 0:\n",
    "        print(f'No publications found in this set for user: {user}')\n",
    "        return pd.DataFrame()\n",
    "    user_publications = publications.loc[publications['id'].isin(user_publications_ids)]\n",
    "    if len(user_publications) == 0:\n",
    "        print(f'No publications found in this set for user: {user}')\n",
    "        return user_publications\n",
    "    print(f'{len(user_publications)} publications found in this set for user: {user}')\n",
    "    return user_publications\n",
    "\n",
    "def get_publications_by_user_name_and_publication_name(publications, authors, users, first_name, last_name, pub_title):\n",
    "    user_publications = get_publications_by_user_name(publications, authors, users, first_name, last_name)\n",
    "    if pub_title and len(pub_title) != 0:\n",
    "        random_single_publication = user_publications.loc[user_publications['title'].str.lower().str.contains(pub_title.lower())]\n",
    "        if len(random_single_publication) == 0:\n",
    "            print(f'No publication found with title: {pub_title}')\n",
    "            return user_publications, pd.DataFrame()\n",
    "        user_publications = user_publications.loc[user_publications['id'] != random_single_publication['id'].values[0]]\n",
    "    else:\n",
    "        print('No publication title provided, returning random publication')\n",
    "        random_single_publication = user_publications.sample(n=1)\n",
    "        user_publications = user_publications.loc[user_publications['id'] != random_single_publication['id'].values[0]]\n",
    "    print(f'Random single publication: {random_single_publication.values[0]}')\n",
    "    return user_publications, random_single_publication\n",
    "\n",
    "def get_user_id_by_user_name(users, first_name, last_name):\n",
    "    if len(first_name) == 0 or len(last_name) == 0:\n",
    "        print('First name or last name is empty, returning full publications set')\n",
    "        return pd.DataFrame()\n",
    "    first_name = first_name.lower()\n",
    "    last_name = last_name.lower()\n",
    "    user = users.loc[(users['first_name'].str.lower().str.contains(first_name)) & (users['last_name'].str.lower().str.contains(last_name))].values[0]\n",
    "    return user[0]\n",
    "\n",
    "def get_publications_by_user_id(publications, authors, user_id):\n",
    "    if user_id is None or user_id == 0:\n",
    "        print('No user id')\n",
    "        return pd.DataFrame()\n",
    "    user_publications_ids = authors.loc[authors['user_id'] == user_id]['publication_id'].values\n",
    "    if len(user_publications_ids) == 0:\n",
    "        print(f'No publications found in this set for user: {user_id}')\n",
    "        return pd.DataFrame()\n",
    "    user_publications = publications.loc[publications['id'].isin(user_publications_ids)]\n",
    "    if len(user_publications) == 0:\n",
    "        print(f'No publications found in this set for user: {user_id}')\n",
    "        return user_publications\n",
    "    print(f'{len(user_publications)} publications found in this set for user: {user_id}')\n",
    "    return user_publications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters for text processing and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 -m spacy download en_core_web_trf\n",
    "lemmatization_model_en = 'en_core_web_trf'\n",
    "# python3 -m spacy download ro_core_news_lg\n",
    "lemmatization_model_ro = 'ro_core_news_lg'\n",
    "\n",
    "stop_words_en = stopwords.words('english')\n",
    "stop_words_en.extend(['from', 'subject', 're', 'edu', 'use', 'result', 'datum'])\n",
    "stop_words_ro = stopwords.words('romanian')\n",
    "stop_words_ro.extend(['tip', 'of', 'the', 'for'])\n",
    "\n",
    "# Set LDA parameters\n",
    "num_topics = 10\n",
    "num_words = 20\n",
    "chunksize = 2000\n",
    "passes = 5\n",
    "alpha = 'asymmetric'\n",
    "eta = 'auto'\n",
    "iterations = 500\n",
    "eval_every = 1\n",
    "workers = 4\n",
    "random_state = 42"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching publications for a specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_first_name = input('Enter author first name: ') \n",
    "user_last_name = input('Enter author last name: ')\n",
    "# Testing specific case in which author published something very different from his usual publications\n",
    "publications_en, random_author_publication_en = get_publications_by_user_name_and_publication_name(publications_all_en, authors_df, users_df, user_first_name, user_last_name, None)\n",
    "publications_en, other_author_publication_en = get_publications_by_user_name_and_publication_name(publications_en, authors_df, users_df, user_first_name, user_last_name, 'Indoor Positioning')\n",
    "publications_ro = get_publications_by_user_name(publications_all_ro, authors_df, users_df, user_first_name, user_last_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(texts):\n",
    "    if len(texts) == 0:\n",
    "        return texts\n",
    "    # Remove punctuation, newlines and tabs\n",
    "    texts = texts.map(lambda x: re.sub('[,\\\\.!?`\\'\\n\\t•„\"\\\\(\\\\)]', '', x))\n",
    "    # Remove numbers\n",
    "    texts = texts.map(lambda x: re.sub('[0-9]', '', x))\n",
    "    # Convert to lowercase\n",
    "    texts = texts.map(lambda x: x.lower())\n",
    "    return texts\n",
    "\n",
    "def split_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts, stop_words):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def preprocess_text(data, lemmatization_model, stop_words):\n",
    "    if len(data) == 0 or len(lemmatization_model) == 0 or len(stop_words) == 0:\n",
    "        return []\n",
    "    \n",
    "    # tokenize\n",
    "    data_words = list(split_words(data))\n",
    "\n",
    "    # Initialize spacy model, keeping only tagger component (for efficiency)\n",
    "    nlp = spacy.load(lemmatization_model, disable=['parser', 'ner'])\n",
    "\n",
    "    # lemmatization\n",
    "    data_words = lemmatization(data_words, nlp)\n",
    "\n",
    "    # remove stop words\n",
    "    data_words = remove_stopwords(data_words, stop_words)\n",
    "\n",
    "    return data_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_and_corpus(data_words):\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    # Create Corpus\n",
    "    texts = data_words\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    return id2word, corpus\n",
    "\n",
    "def build_lda_model(corpus, id2word, num_topics_param, num_words_param):\n",
    "    if len(corpus) == 0 or len(id2word) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Build LDA model\n",
    "    lda_model = LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=num_topics_param,\n",
    "        chunksize=chunksize,\n",
    "        passes=passes,\n",
    "        alpha=alpha,\n",
    "        eta=eta,\n",
    "        eval_every=eval_every,\n",
    "        iterations=iterations,\n",
    "        workers=workers,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    doc_lda = None\n",
    "    if lda_model is not None:\n",
    "        if DEBUG:\n",
    "            pprint(lda_model.print_topics(num_topics=num_topics_param, num_words=num_words_param))\n",
    "        doc_lda = lda_model[corpus]\n",
    "\n",
    "    return lda_model, doc_lda\n",
    "\n",
    "def create_lda_for_publications(publications, lemmatization_model, stop_words, num_topics_param, num_words_param):\n",
    "    publications['abstract_text_processed'] = cleanup_text(publications['abstract_text'])\n",
    "    if DEBUG:\n",
    "        print(publications['abstract_text_processed'].head())\n",
    "\n",
    "    data = publications['abstract_text_processed'].values.tolist()\n",
    "\n",
    "    data_words = preprocess_text(data, lemmatization_model, stop_words)\n",
    "    if DEBUG:\n",
    "        print(data_words[:1])\n",
    "\n",
    "    id2word, corpus = create_dictionary_and_corpus(data_words)\n",
    "\n",
    "    if DEBUG:\n",
    "        print(corpus[:1])\n",
    "        print([[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]])\n",
    "\n",
    "    lda_model, doc_lda = build_lda_model(corpus, id2word, num_topics_param, num_words_param)\n",
    "\n",
    "    return lda_model, doc_lda, id2word, corpus, data_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_en, doc_lda_en, id2word_en, corpus_en, data_words_en = create_lda_for_publications(publications_en, lemmatization_model_en, stop_words_en, num_topics, num_words)\n",
    "lda_model_ro, doc_lda_ro, id2word_ro, corpus_ro, data_words_ro = create_lda_for_publications(publications_ro, lemmatization_model_ro, stop_words_ro, num_topics, num_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_wordcloud(data_words):\n",
    "    if len(data_words) == 0:\n",
    "        data_words = [['none']]\n",
    "    # Join the different processed titles together.\n",
    "    long_string = ','.join([word for sublist in data_words for word in sublist])\n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue', width=800, height=600)\n",
    "    # Generate a word cloud\n",
    "    wordcloud.generate(long_string)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_wordcloud(data_words_en).to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_wordcloud(data_words_ro).to_image()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic metrics and topic visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(lda_model, model_description, corpus, id2word, data_words):\n",
    "    if lda_model is None or len(corpus) == 0 or len(id2word) == 0 or len(data_words) == 0:\n",
    "        print(f'No metrics for {model_description}\\n')\n",
    "        return\n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f'Coherence Score for {model_description}: {coherence_lda}\\n')\n",
    "\n",
    "compute_metrics(lda_model_en, 'EN', corpus_en, id2word_en, data_words_en)\n",
    "compute_metrics(lda_model_ro, 'RO', corpus_ro, id2word_ro, data_words_ro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_en, corpus_en, id2word_en, mds='mmds')\n",
    "vis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal number of topics for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(id2word, corpus, texts, num_topics_range):\n",
    "    coherence_values = []\n",
    "    model_data_list = []\n",
    "    for num_topics_param in num_topics_range:\n",
    "        if num_topics_param == 0:\n",
    "            num_topics_param = 1\n",
    "        model_data = build_lda_model(corpus, id2word, num_topics_param, num_words)\n",
    "        model_data_list.append(model_data)\n",
    "        coherencemodel = CoherenceModel(model=model_data[0], texts=texts, dictionary=id2word, coherence='c_v')\n",
    "        coherence_values.append(round(coherencemodel.get_coherence(), 2))\n",
    "\n",
    "    return model_data_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_best_num_topics(id2word, corpus, data_words):\n",
    "    start = 0\n",
    "    limit = 30\n",
    "    step = 5\n",
    "    num_topics_range = [1 if i == 0 else i for i in range(start, limit, step)]\n",
    "    model_list, coherence_values = compute_coherence_values(id2word, corpus, data_words, num_topics_range)\n",
    "    best_model, best_doc_lda = model_list[coherence_values.index(max(coherence_values))]\n",
    "    return best_model, best_doc_lda, coherence_values, num_topics_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_en, doc_lda_en, coherence_values_en, num_topics_range = compute_best_num_topics(id2word_en, corpus_en, data_words_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "plt.plot(num_topics_range, coherence_values_en)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(num_topics_range, coherence_values_en):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = max(list(zip(num_topics_range, coherence_values_en)), key=lambda x: x[1])[0]\n",
    "print(f'Optimal number of topics = {num_topics}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding out the dominant topic for each doc in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dominant_topics(lda_model, doc_lda, texts):\n",
    "    sent_topics_df = pd.DataFrame(columns=['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text'])\n",
    "\n",
    "    for i, row in enumerate(doc_lda):\n",
    "        dominant_topic = max(row, key=lambda x: x[1])\n",
    "        dominant_topic_id = dominant_topic[0]\n",
    "        dominant_topic_contrib_perc = dominant_topic[1]\n",
    "\n",
    "        topic = lda_model.show_topic(dominant_topic[0])\n",
    "        topic_keywords = \",\".join([word for word, _ in topic])\n",
    "        sent_topics_df.loc[i] = [i, dominant_topic_id, dominant_topic_contrib_perc, topic_keywords, texts[i]]\n",
    "\n",
    "    return sent_topics_df\n",
    "\n",
    "\n",
    "df_dominant_topic = get_dominant_topics(lda_model_en, doc_lda_en, data_words_en)\n",
    "\n",
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing a new paper to the author's corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_single_publication(random_single_publication, model_to_compare, lemmatization_model, stop_words, distance):\n",
    "    lda_model, _, _, _, _ = create_lda_for_publications(random_single_publication, lemmatization_model, stop_words, num_topics, num_words)\n",
    "\n",
    "    if lda_model is not None:\n",
    "        if DEBUG:\n",
    "            pprint(lda_model.print_topics(num_topics=num_topics, num_words=num_words))\n",
    "            pprint(model_to_compare.print_topics(num_topics=num_topics, num_words=num_words))\n",
    "        mdiff, annotation = lda_model.diff(model_to_compare, distance=distance, num_words=num_words, n_ann_terms=num_words)\n",
    "        if DEBUG:\n",
    "            print(mdiff)\n",
    "            print(annotation)\n",
    "        return mdiff, annotation, np.mean(mdiff)\n",
    "    else:\n",
    "        print('No model')\n",
    "        return None, None, None\n",
    "\n",
    "def plot_difference(mdiff, title=\"\"):\n",
    "    _, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.set_xlabel('Topics for new paper')\n",
    "    ax.set_ylabel('Topics for author corpus')\n",
    "    data = ax.imshow(mdiff, cmap='RdBu_r', origin='lower', vmin=0.75, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PICKED_PAPERS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_PICKED_PAPERS:\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/lda_model_{user_first_name}_{user_last_name}.pkl', 'rb') as f:\n",
    "        lda_model_en = pickle.load(f)\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/random_author_publication_en.pkl', 'rb') as f:\n",
    "        random_author_publication_en = pickle.load(f)\n",
    "    try:\n",
    "        with open(f'data/{user_first_name}_{user_last_name}/other_author_publication_en.pkl', 'rb') as f:\n",
    "            other_author_publication_en = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        other_author_publication_en = None\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/publications_en.pkl', 'rb') as f:\n",
    "        publications_en = pickle.load(f)\n",
    "\n",
    "    _, _, id2word_en, corpus_en, data_words_en = create_lda_for_publications(publications_en, lemmatization_model_en, stop_words_en, num_topics, num_words)\n",
    "    doc_lda_en = lda_model_en[corpus_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PICKED_PAPERS = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_PICKED_PAPERS:\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/lda_model_{user_first_name}_{user_last_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(lda_model_en, f)\n",
    "    random_author_publication_en.to_pickle(f'data/{user_first_name}_{user_last_name}/random_author_publication_en.pkl')\n",
    "    if other_author_publication_en is not None and len(other_author_publication_en) > 0:\n",
    "        other_author_publication_en.to_pickle(f'data/{user_first_name}_{user_last_name}/other_author_publication_en.pkl')\n",
    "    publications_en.to_pickle(f'data/{user_first_name}_{user_last_name}/publications_en.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Picked random paper from same author:\\nid={random_author_publication_en['id'].values[0]} {random_author_publication_en['title'].values[0]}\\nfrom authors:\\n{random_author_publication_en['authors'].values[0]}\")\n",
    "mdiff, annotation, mdiff_mean = compare_single_publication(random_author_publication_en, lda_model_en, lemmatization_model_en, stop_words_en, 'jaccard')\n",
    "plot_difference(mdiff, title=\"Topic difference (random paper from same author) [jaccard distance]\")\n",
    "print(f\"Mean difference: {mdiff_mean}\")\n",
    "\n",
    "if other_author_publication_en is not None and len(other_author_publication_en) > 0:\n",
    "    print(f\"Picked other paper from same author:\\nid={other_author_publication_en['id'].values[0]} {other_author_publication_en['title'].values[0]}\\nfrom authors:\\n{other_author_publication_en['authors'].values[0]}\")\n",
    "    mdiff, annotation, mdiff_mean = compare_single_publication(other_author_publication_en, lda_model_en, lemmatization_model_en, stop_words_en, 'jaccard')\n",
    "    plot_difference(mdiff, title=\"Topic difference (random paper from same author) [jaccard distance]\")\n",
    "    print(f\"Mean difference: {mdiff_mean}\")\n",
    "\n",
    "random_single_publication_en = publications_all_en.drop(publications_en.index).sample(n=1)\n",
    "print(f\"Picked random paper from full dataset:\\n{random_single_publication_en['title'].values[0]}\\nfrom authors:\\n{random_single_publication_en['authors'].values[0]}\")\n",
    "mdiff, annotation, mdiff_mean = compare_single_publication(random_single_publication_en, lda_model_en, lemmatization_model_en, stop_words_en, 'jaccard')\n",
    "plot_difference(mdiff, title=\"Topic difference (random paper from dataset) [jaccard distance]\")\n",
    "print(f\"Mean difference: {mdiff_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_en, corpus_en, id2word_en, mds='mmds')\n",
    "vis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification for LDA topic diffs for chosen author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_svm(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    f1_score_result = round(f1_score(y_test, y_pred), 2)\n",
    "    accuracy_score_result = round(accuracy_score(y_test, y_pred), 2)\n",
    "    return clf, scaler, f1_score_result, accuracy_score_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_author_lda_dataset(publications, other_publications, sample_size):\n",
    "    author_publications = publications.sample(n=sample_size)\n",
    "    author_publications = author_publications.reset_index(drop=True)\n",
    "    non_author_publications = other_publications[~other_publications['id'].isin(author_publications['id'])]\n",
    "    author_lda_model, _, _, _, _ = create_lda_for_publications(author_publications, lemmatization_model_en, stop_words_en, num_topics, num_words)\n",
    "    train_set_size = 2 * len(author_publications)\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(train_set_size):\n",
    "        if i < len(author_publications):\n",
    "            author_publication = author_publications.loc[i, :].to_frame().T\n",
    "            other_author_publications = author_publications.drop(author_publication.index)\n",
    "            other_author_lda_model, _, _, _, _ = create_lda_for_publications(other_author_publications, lemmatization_model_en, stop_words_en, num_topics, num_words) \n",
    "            mdiff, _, _ = compare_single_publication(author_publication, other_author_lda_model, lemmatization_model_en, stop_words_en, 'jaccard')\n",
    "            if mdiff is not None:\n",
    "                x.append(mdiff.flatten())\n",
    "                y.append(1)\n",
    "        else:\n",
    "            random_non_author_publication_en = non_author_publications.sample(n=1)\n",
    "            non_author_publications = non_author_publications.drop(random_non_author_publication_en.index)\n",
    "            mdiff, _, _ = compare_single_publication(random_non_author_publication_en, author_lda_model, lemmatization_model_en, stop_words_en, 'jaccard')\n",
    "            if mdiff is not None:\n",
    "                x.append(mdiff.flatten())\n",
    "                y.append(0) \n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_author_lda_dataset(publications_en, publications_all_en, len(publications_en))\n",
    "\n",
    "if SAVE:\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/lda_diff_{user_first_name}_{user_last_name}_x.pickle', 'wb') as f:\n",
    "        pickle.dump(x, f)\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/lda_diff_{user_first_name}_{user_last_name}_y.pickle', 'wb') as f:\n",
    "        pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/lda_diff_{user_first_name}_{user_last_name}_x.pickle', 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/lda_diff_{user_first_name}_{user_last_name}_y.pickle', 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Run previous cell first')\n",
    "\n",
    "clf, scaler, f1_score_result, accuracy_score_result = train_and_test_svm(x, y)\n",
    "print(f\"F1 score: {f1_score_result}\")\n",
    "print(f\"Accuracy score: {accuracy_score_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Picked random paper from same author:\\nid={random_author_publication_en['id'].values[0]} {random_author_publication_en['title'].values[0]}\\nfrom authors:\\n{random_author_publication_en['authors'].values[0]}\")\n",
    "mdiff, _, _ = compare_single_publication(random_author_publication_en, lda_model_en, lemmatization_model_en, stop_words_en, 'jaccard')\n",
    "pred = clf.predict(scaler.transform(mdiff.flatten().reshape(1, -1)))\n",
    "print(f\"Prediction: {'' if pred[0] == 1 else 'not '}same author\")\n",
    "\n",
    "if other_author_publication_en is not None and len(other_author_publication_en) > 0:\n",
    "    print(f\"Picked other random paper from same author:\\nid={other_author_publication_en['id'].values[0]} {other_author_publication_en['title'].values[0]}\\nfrom authors:\\n{other_author_publication_en['authors'].values[0]}\")\n",
    "    mdiff, _, _ = compare_single_publication(other_author_publication_en, lda_model_en, lemmatization_model_en, stop_words_en, 'jaccard')\n",
    "    pred = clf.predict(scaler.transform(mdiff.flatten().reshape(1, -1)))\n",
    "    print(f\"Prediction: {'' if pred[0] == 1 else 'not '}same author\")\n",
    "\n",
    "print(f\"Picked random paper from full dataset:\\n{random_single_publication_en['title'].values[0]}\\nfrom authors:\\n{random_single_publication_en['authors'].values[0]}\")\n",
    "mdiff, _, _ = compare_single_publication(random_single_publication_en, lda_model_en, lemmatization_model_en, stop_words_en, 'jaccard')\n",
    "pred = clf.predict(scaler.transform(mdiff.flatten().reshape(1, -1)))\n",
    "print(f\"Prediction: {'' if pred[0] == 1 else 'not '}same author\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification for LDA topic diffs using one classifier per author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to this step taking a long time to run, the number of authors selected and publications per author has been reduced.\n",
    "This section should also be run only once and the results should be loaded from the pickle file for subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_publications = []\n",
    "for user_id in users_df['id'].values:\n",
    "    user_publications = get_publications_by_user_id(publications_all_en, authors_df, user_id)\n",
    "    if len(user_publications) > 0:\n",
    "        users_publications.append((user_id, user_publications))\n",
    "\n",
    "print(f'Found {len(users_publications)} users with publications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_users_publications = list(filter(lambda x: len(x[1]) > 20, users_publications))\n",
    "len(filtered_users_publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_author_classifier_results = pd.DataFrame(columns=['author_id', 'f1_score', 'accuracy_score'])\n",
    "\n",
    "all_author_x = []\n",
    "all_author_y = []\n",
    "\n",
    "for user_data in tqdm(filtered_users_publications):\n",
    "    x, y = create_author_lda_dataset(user_data[1], publications_all_en, 10)\n",
    "\n",
    "    _, _, f1_score_result, accuracy_score_result = train_and_test_svm(x, y)\n",
    "    per_author_classifier_results.loc[len(per_author_classifier_results)] = [user_data[0], f1_score_result, accuracy_score_result]\n",
    "    all_author_x.extend(x)\n",
    "    all_author_y.extend(y)\n",
    "\n",
    "if SAVE:\n",
    "    per_author_classifier_results.to_csv('data/per_author_lda_classifier_results.csv', index=False)\n",
    "    with open('data/lda_diff_all_author_x.pickle', 'wb') as f:\n",
    "        pickle.dump(all_author_x, f)\n",
    "    with open('data/lda_diff_all_author_y.pickle', 'wb') as f:\n",
    "        pickle.dump(all_author_y, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification for LDA topic diffs using one classifier for all authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('data/lda_diff_all_author_x.pickle', 'rb') as f:\n",
    "        all_author_x = pickle.load(f)\n",
    "    with open('data/lda_diff_all_author_y.pickle', 'rb') as f:\n",
    "        all_author_y = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Run previous cell first')\n",
    "\n",
    "lda_all_author_clf, lda_all_author_scaler, f1_score_result, accuracy_score_result = train_and_test_svm(all_author_x, all_author_y)\n",
    "print(f'Classifier results for all authors: f1_score = {f1_score_result} accuracy_score = {accuracy_score_result}')\n",
    "all_author_classifier_results = pd.DataFrame(columns=['f1_score', 'accuracy_score'])\n",
    "all_author_classifier_results.loc[len(all_author_classifier_results)] = [f1_score_result, accuracy_score_result]\n",
    "all_author_classifier_results.to_csv('data/all_author_classifier_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    with open(f'data/lda_all_author_clf.pickle', 'wb') as f:\n",
    "        pickle.dump(lda_all_author_clf, f)\n",
    "    with open(f'data/lda_all_author_scaler.pickle', 'wb') as f:\n",
    "        pickle.dump(lda_all_author_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('data/lda_all_author_clf.pickle', 'rb') as f:\n",
    "        lda_all_author_clf = pickle.load(f)\n",
    "    with open('data/lda_all_author_scaler.pickle', 'rb') as f:\n",
    "        lda_all_author_scaler = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Run previous cell first')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword extraction using KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_model_en = KeyBERT()\n",
    "keyphrase_ngram_range = (1, 1)\n",
    "num_keywords = 20\n",
    "author_keywords = kw_model_en.extract_keywords(publications_en['abstract_text'].values, keyphrase_ngram_range=keyphrase_ngram_range, stop_words=\"english\", top_n=num_keywords)\n",
    "print(author_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_wordcloud([[x[0] for x in sub] for sub in author_keywords]).to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_author_publication_keywords = kw_model_en.extract_keywords(random_author_publication_en['abstract_text'].values, keyphrase_ngram_range=keyphrase_ngram_range, stop_words=\"english\", top_n=num_keywords)\n",
    "print(random_author_publication_keywords)\n",
    "if other_author_publication_en is not None and len(other_author_publication_en) > 0:\n",
    "    other_random_author_publication_keywords = kw_model_en.extract_keywords(other_author_publication_en['abstract_text'].values, keyphrase_ngram_range=keyphrase_ngram_range, stop_words=\"english\", top_n=num_keywords)\n",
    "    print(other_random_author_publication_keywords)\n",
    "random_single_publication_keywords = kw_model_en.extract_keywords(random_single_publication_en['abstract_text'].values, keyphrase_ngram_range=keyphrase_ngram_range, stop_words=\"english\", top_n=num_keywords)\n",
    "print(random_single_publication_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_wordcloud([[x[0] for x in random_author_publication_keywords]]).to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_wordcloud([[x[0] for x in other_random_author_publication_keywords]] if other_author_publication_en is not None and len(other_author_publication_en) > 0 else []).to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_wordcloud([[x[0] for x in random_single_publication_keywords]]).to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the simmilarity score based on keywords and weights\n",
    "\n",
    "We will use the formula proposed by Courtney Corley and Rada Mihalcea in Measuring the Semantic Similarity of Texts and modify it to use the contextualized word embeddings from BERT.\n",
    "First we will compute keyword overlap between two documents and then relatedness between overlapping keywords in those documents.\n",
    "Relatedness will be computed using cosine similarity of word embeddings for overlapping keywords. \n",
    "The similarity score will be calculated by calculating the sum of all keyword relatedness values.\n",
    "When comparing a new publication with the author's corpus, we will compute the similarity score to each publication in the corpus and compute the average of those scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_overlap(keywords1, keywords2):\n",
    "    keywords_set1 = set(keywords1)\n",
    "    keywords_set2 = set(keywords2)\n",
    "    intersection = keywords_set1.intersection(keywords_set2)\n",
    "    return len(intersection), [keywords1.index(k) for k in intersection], [keywords2.index(k) for k in intersection]\n",
    "\n",
    "def compute_word_relatedness(embeddings1, embeddings2):\n",
    "    return np.dot(embeddings1, embeddings2) / (np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2))\n",
    "\n",
    "def compute_doc_similarity(doc1, doc2, kw_model, keyphrase_ngram_range, stop_words):\n",
    "    keywords1 = kw_model.extract_keywords(doc1, keyphrase_ngram_range=keyphrase_ngram_range, stop_words=stop_words, top_n=num_keywords)\n",
    "    keywords2 = kw_model.extract_keywords(doc2, keyphrase_ngram_range=keyphrase_ngram_range, stop_words=stop_words, top_n=num_keywords)\n",
    "    keywords1 = [k[0] for k in keywords1]\n",
    "    keywords2 = [k[0] for k in keywords2]\n",
    "    _, embeddings1 = kw_model.extract_embeddings(doc1, candidates=keywords1, keyphrase_ngram_range=keyphrase_ngram_range, stop_words=stop_words)\n",
    "    _, embeddings2 = kw_model.extract_embeddings(doc2, candidates=keywords2, keyphrase_ngram_range=keyphrase_ngram_range, stop_words=stop_words)\n",
    "    word_overlap, overalp_ids1, overlap_ids2 = compute_word_overlap(keywords1, keywords2)\n",
    "    if word_overlap == 0:\n",
    "        return 0\n",
    "    overlap_embeddings1 = [embeddings1[i] for i in overalp_ids1]\n",
    "    overlap_embeddings2 = [embeddings2[i] for i in overlap_ids2]\n",
    "    word_relatedness_scores = [compute_word_relatedness(e1, e2) for e1, e2 in zip(overlap_embeddings1, overlap_embeddings2)]\n",
    "    return sum(word_relatedness_scores)\n",
    "\n",
    "def compute_publication_similarity_with_author(publication, author_publications, kw_model, keyphrase_ngram_range, stop_words):\n",
    "    publication_similarity_scores = []\n",
    "    publication_text = publication['abstract_text'].values[0]\n",
    "    for author_publication_text in author_publications['abstract_text'].values:\n",
    "        publication_similarity_scores.append(compute_doc_similarity(publication_text, author_publication_text, kw_model, keyphrase_ngram_range, stop_words))\n",
    "    return np.mean(publication_similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = compute_publication_similarity_with_author(random_author_publication_en, publications_en, kw_model_en, keyphrase_ngram_range, \"english\")\n",
    "print(f'Similarity for publication {random_author_publication_en[\"id\"].values[0]} with title {random_author_publication_en[\"title\"].values[0]} = {similarity}')\n",
    "\n",
    "if other_author_publication_en is not None and len(other_author_publication_en) > 0:\n",
    "    similarity = compute_publication_similarity_with_author(other_author_publication_en, publications_en, kw_model_en, keyphrase_ngram_range, \"english\")\n",
    "    print(f'Similarity for publication {other_author_publication_en[\"id\"].values[0]} with title {other_author_publication_en[\"title\"].values[0]} = {similarity}')\n",
    "\n",
    "similarity = compute_publication_similarity_with_author(random_single_publication_en, publications_en, kw_model_en, keyphrase_ngram_range, \"english\")\n",
    "print(f'Similarity for publication {random_single_publication_en[\"id\"].values[0]} with title {random_single_publication_en[\"title\"].values[0]} = {similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_author_keyword_dataset(kw_model, keyphrase_ngram_range_param, publications, other_publications, sample_size):\n",
    "    author_publications = publications.sample(n=sample_size)\n",
    "    author_publications = author_publications.reset_index(drop=True)\n",
    "    non_author_publications = other_publications[~other_publications['id'].isin(author_publications['id'])]\n",
    "    train_set_size = 2 * len(author_publications)\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(train_set_size):\n",
    "        if i < len(author_publications):\n",
    "            author_publication = author_publications.loc[i, :].to_frame().T\n",
    "            other_author_publications = author_publications.drop(author_publication.index)\n",
    "            similarity = compute_publication_similarity_with_author(author_publication, other_author_publications, kw_model, keyphrase_ngram_range_param, \"english\")\n",
    "            x.append(similarity)\n",
    "            y.append(1)\n",
    "        else:\n",
    "            random_non_author_publication_en = non_author_publications.sample(n=1)\n",
    "            similarity = compute_publication_similarity_with_author(random_non_author_publication_en, author_publications, kw_model, keyphrase_ngram_range_param, \"english\")\n",
    "            x.append(similarity)\n",
    "            y.append(0)\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of various pre-trained BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_models = [\n",
    "    'all-mpnet-base-v2',\n",
    "    'all-distilroberta-v1',\n",
    "    'multi-qa-distilbert-cos-v1',\n",
    "    'all-MiniLM-L12-v2',\n",
    "    'all-MiniLM-L6-v2'\n",
    "]\n",
    "\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for bert_model in bert_models:\n",
    "    kw_model = KeyBERT(bert_model)\n",
    "    x, y = create_author_keyword_dataset(kw_model, keyphrase_ngram_range, publications_en, publications_all_en, len(publications_en))\n",
    "    x = np.array(x).reshape(-1, 1)\n",
    "\n",
    "    _, _, f1_score_result, accuracy_score_result = train_and_test_svm(x, y)\n",
    "    f1_scores.append(f1_score_result)\n",
    "    accuracy_scores.append(accuracy_score_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(bert_models, f1_scores, label='F1 score')\n",
    "plt.title('F1 scores for different BERT models')\n",
    "plt.xlabel('BERT model')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(bert_models, accuracy_scores, label='Accuracy score')\n",
    "plt.title('Accuracy scores for different BERT models')\n",
    "plt.xlabel('BERT model')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "kw_model_en = KeyBERT(bert_models[np.argmax(f1_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using monograms, bigrams or trigrams for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrase_ngram_ranges = [(1, 3), (1, 2), (1, 1)]\n",
    "\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for ngram_range in keyphrase_ngram_ranges:\n",
    "    x, y = create_author_keyword_dataset(kw_model_en, ngram_range, publications_en, publications_all_en, len(publications_en))\n",
    "    x = np.array(x).reshape(-1, 1)\n",
    "\n",
    "    _, _, f1_score_result, accuracy_score_result = train_and_test_svm(x, y)\n",
    "    repeat = 0\n",
    "    while (f1_score_result == 1 or accuracy_score_result == 1) and repeat < 10:\n",
    "       _, _, f1_score_result, accuracy_score_result = train_and_test_svm(x, y)\n",
    "       repeat += 1\n",
    "    f1_scores.append(f1_score_result)\n",
    "    accuracy_scores.append(accuracy_score_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(list(map(lambda x: str(x), keyphrase_ngram_ranges)), f1_scores, label='F1 score')\n",
    "plt.title('F1 scores for different BERT models')\n",
    "plt.xlabel('BERT model')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(list(map(lambda x: str(x), keyphrase_ngram_ranges)), accuracy_scores, label='Accuracy score')\n",
    "plt.title('Accuracy scores for different BERT models')\n",
    "plt.xlabel('BERT model')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "keyphrase_ngram_range = keyphrase_ngram_ranges[np.argmax(f1_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    with open(f'data/keybert_model.pickle', 'wb') as f:\n",
    "        pickle.dump(kw_model_en, f)\n",
    "    with open(f'data/keyphrase_ngram_range.pickle', 'wb') as f:\n",
    "        pickle.dump(keyphrase_ngram_range, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(f'data/keybert_model.pickle', 'rb') as f:\n",
    "        kw_model_en = pickle.load(f)\n",
    "    with open(f'data/keyphrase_ngram_range.pickle', 'rb') as f:\n",
    "        keyphrase_ngram_range = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Run previous cell first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification of similarity scores computed for chosen author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = create_author_keyword_dataset(kw_model_en, keyphrase_ngram_range, publications_en, publications_all_en, len(publications_en))\n",
    "x = np.array(x).reshape(-1, 1)\n",
    "\n",
    "if SAVE:\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/keyword_similarity_{user_first_name}_{user_last_name}_x.pickle', 'wb') as f:\n",
    "        pickle.dump(x, f)\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/keyword_similarity_{user_first_name}_{user_last_name}_y.pickle', 'wb') as f:\n",
    "        pickle.dump(y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/keyword_similarity_{user_first_name}_{user_last_name}_x.pickle', 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    with open(f'data/{user_first_name}_{user_last_name}/keyword_similarity_{user_first_name}_{user_last_name}_y.pickle', 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Run previous cell first')\n",
    "\n",
    "clf, scaler, f1_score_result, accuracy_score_result = train_and_test_svm(x, y)\n",
    "print(f\"F1 score: {f1_score_result}\")\n",
    "print(f\"Accuracy score: {accuracy_score_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Picked random paper from same author:\\nid={random_author_publication_en['id'].values[0]} {random_author_publication_en['title'].values[0]}\\nfrom authors:\\n{random_author_publication_en['authors'].values[0]}\")\n",
    "similarity = compute_publication_similarity_with_author(random_author_publication_en, publications_en, kw_model_en, keyphrase_ngram_range, \"english\")\n",
    "pred = clf.predict(scaler.transform(np.array(similarity).reshape(1, -1)))\n",
    "print(f\"Prediction: {'' if pred[0] == 1 else 'not '}same author\")\n",
    "\n",
    "if other_author_publication_en is not None and len(other_author_publication_en) > 0:\n",
    "    print(f\"Picked other random paper from same author:\\nid={other_author_publication_en['id'].values[0]} {other_author_publication_en['title'].values[0]}\\nfrom authors:\\n{other_author_publication_en['authors'].values[0]}\")\n",
    "    similarity = compute_publication_similarity_with_author(other_author_publication_en, publications_en, kw_model_en, keyphrase_ngram_range, \"english\")\n",
    "    pred = clf.predict(scaler.transform(np.array(similarity).reshape(1, -1)))\n",
    "    print(f\"Prediction: {'' if pred[0] == 1 else 'not '}same author\")\n",
    "\n",
    "print(f\"Picked random paper from full dataset:\\n{random_single_publication_en['title'].values[0]}\\nfrom authors:\\n{random_single_publication_en['authors'].values[0]}\")\n",
    "similarity = compute_publication_similarity_with_author(random_single_publication_en, publications_en, kw_model_en, keyphrase_ngram_range, \"english\")\n",
    "pred = clf.predict(scaler.transform(np.array(similarity).reshape(1, -1)))\n",
    "print(f\"Prediction: {'' if pred[0] == 1 else 'not '}same author\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification of similarity scores using one classifier per author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_author_classifier_results = pd.DataFrame(columns=['author_id', 'f1_score', 'accuracy_score'])\n",
    "\n",
    "all_author_x = np.empty((0, 1))\n",
    "all_author_y = []\n",
    "\n",
    "for user_data in tqdm(filtered_users_publications):\n",
    "    x, y = create_author_keyword_dataset(kw_model_en, keyphrase_ngram_range, user_data[1], publications_all_en, 10)\n",
    "    x = np.array(x).reshape(-1, 1)\n",
    "\n",
    "    _, _, f1_score_result, accuracy_score_result = train_and_test_svm(x, y)\n",
    "    per_author_classifier_results.loc[len(per_author_classifier_results)] = [user_data[0], f1_score_result, accuracy_score_result]\n",
    "    all_author_x = np.concatenate((all_author_x, x), axis=0)\n",
    "    all_author_y.extend(y)\n",
    "\n",
    "if SAVE:\n",
    "    per_author_classifier_results.to_csv('data/per_author_keyword_classifier_results.csv', index=False)\n",
    "    with open('data/keyword_similarity_all_author_x.pickle', 'wb') as f:\n",
    "        pickle.dump(all_author_x, f)\n",
    "    with open('data/keyword_similarity_all_author_y.pickle', 'wb') as f:\n",
    "        pickle.dump(all_author_y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification of similarity scores using one classifier for all authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('data/keyword_similarity_all_author_x.pickle', 'rb') as f:\n",
    "        all_author_x = pickle.load(f)\n",
    "    with open('data/keyword_similarity_all_author_y.pickle', 'rb') as f:\n",
    "        all_author_y = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Run previous cell first')\n",
    "\n",
    "keybert_all_author_clf, keybert_all_author_scaler, f1_score_result, accuracy_score_result = train_and_test_svm(all_author_x, all_author_y)\n",
    "print(f'Classifier results for all authors: f1_score = {f1_score_result} accuracy_score = {accuracy_score_result}')\n",
    "all_author_classifier_results = pd.DataFrame(columns=['f1_score', 'accuracy_score'])\n",
    "all_author_classifier_results.loc[len(all_author_classifier_results)] = [f1_score_result, accuracy_score_result]\n",
    "all_author_classifier_results.to_csv('data/all_author_classifier_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    with open('data/keybert_all_author_clf.pickle', 'wb') as f:\n",
    "        pickle.dump(keybert_all_author_clf, f)\n",
    "    with open('data/keybert_all_author_scaler.pickle', 'wb') as f:\n",
    "        pickle.dump(keybert_all_author_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('data/keybert_all_author_clf.pickle', 'rb') as f:\n",
    "        keybert_all_author_clf = pickle.load(f)\n",
    "    with open('data/keybert_all_author_scaler.pickle', 'rb') as f:\n",
    "        keybert_all_author_scaler = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Run previous cell first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_first_name = input('Enter first name of author to test: ')\n",
    "test_last_name = input('Enter last name of author to test: ')\n",
    "\n",
    "test_publications_en = get_publications_by_user_name(publications_all_en, authors_df, users_df, test_first_name, test_last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title = input('Enter title of publication to test: ')\n",
    "test_abstract = input('Enter abstract of publication to test: ')\n",
    "\n",
    "cols = ['id', 'title', 'abstract_text', 'keywords', 'authors']\n",
    "vals = [0, test_title, test_abstract, '', '']\n",
    "test_random_author_publication_en = pd.DataFrame([vals], columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lda_model_en, test_doc_lda_en, test_id2word_en, test_corpus_en, test_data_words_en = create_lda_for_publications(test_publications_en, lemmatization_model_en, stop_words_en, num_topics, num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(test_lda_model_en, test_corpus_en, test_id2word_en, mds='mmds')\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Picked random paper: {test_random_author_publication_en['title'].values[0]}\")\n",
    "mdiff, annotation, mdiff_mean = compare_single_publication(test_random_author_publication_en, test_lda_model_en, lemmatization_model_en, stop_words_en, 'jaccard')\n",
    "plot_difference(mdiff, title=\"Topic difference [jaccard distance]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lda_all_author_clf.predict(lda_all_author_scaler.transform(mdiff.flatten().reshape(1, -1)))\n",
    "print(f\"Prediction: {'' if pred[0] == 1 else 'not '}same author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(f'data/{test_first_name}_{test_last_name}/lda_diff_{test_first_name}_{test_last_name}_x.pickle', 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    with open(f'data/{test_first_name}_{test_last_name}/lda_diff_{test_first_name}_{test_last_name}_y.pickle', 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Author not preloaded')\n",
    "\n",
    "lda_author_clf, lda_author_scaler, _, _ = train_and_test_svm(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lda_author_clf.predict(lda_author_scaler.transform(mdiff.flatten().reshape(1, -1)))\n",
    "print(f\"Prediction: {'' if pred[0] == 1 else 'not '}same author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_random_author_publication_keywords = kw_model_en.extract_keywords(test_random_author_publication_en['abstract_text'].values, keyphrase_ngram_range=keyphrase_ngram_range, stop_words=\"english\", top_n=num_keywords)\n",
    "display_wordcloud([[x[0] for x in test_random_author_publication_keywords]]).to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = compute_publication_similarity_with_author(test_random_author_publication_en, test_publications_en, kw_model_en, keyphrase_ngram_range, \"english\")\n",
    "print(f'Similarity for publication {test_random_author_publication_en[\"title\"].values[0]} = {similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = keybert_all_author_clf.predict(keybert_all_author_scaler.transform(np.array(similarity).reshape(1, -1)))\n",
    "print(f\"Prediction: {'' if pred[0] == 1 else 'not '}same author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(f'data/{test_first_name}_{test_last_name}/keyword_similarity_{test_first_name}_{test_last_name}_x.pickle', 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    with open(f'data/{test_first_name}_{test_last_name}/keyword_similarity_{test_first_name}_{test_last_name}_y.pickle', 'rb') as f:\n",
    "        y = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print('Author not preloaded')\n",
    "\n",
    "keybert_author_clf, keybert_author_scaler, _, _ = train_and_test_svm(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = keybert_author_clf.predict(keybert_author_scaler.transform(np.array(similarity).reshape(1, -1)))\n",
    "print(f\"Prediction: {'' if pred[0] == 1 else 'not '}same author\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
