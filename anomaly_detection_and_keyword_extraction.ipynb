{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import pyLDAvis.gensim\n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "nltk.download('stopwords')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications_df = pd.read_csv('data/publications-converted.csv', quotechar=\"'\", delimiter=',', quoting=2)\n",
    "authors_df = pd.read_csv('data/authors_all.csv')\n",
    "users_df = pd.read_csv('data/users_all.csv')\n",
    "users_validated_df = pd.read_csv('data/users_validatAcceptat.csv')\n",
    "\n",
    "publications_df = publications_df.drop(columns=['no_coauthors', 'publication_type', 'no_pages', 'd_oi', \n",
    "                                                'category', 'file_link', 'external_link', 'publisher', 'w_os',\n",
    "                                                'jhi_type', 'cross_ref_validation', 'publication_date',\n",
    "                                                'file_link_shown', 'citations_number', 'metadata', 'internal_link',\n",
    "                                                'keywords_valid', 'photo_link', 'mapped_to_id', 'local_keywords',\n",
    "                                                'keywords_isi'])\n",
    "publications_df = publications_df.dropna(subset=['title', 'abstract_text', 'keywords'])\n",
    "langs = publications_df['abstract_lang'].unique()\n",
    "print(f'Starting languages: {langs}')\n",
    "# Only one publication in 'UNKNOWN' language and it is in english\n",
    "publications_df.loc[publications_df['abstract_lang'] == 'UNKNOWN', 'abstract_lang'] = 'en'\n",
    "# Drop publications with no abstract language\n",
    "publications_df = publications_df.dropna(subset=['abstract_lang'])\n",
    "langs = publications_df['abstract_lang'].unique()\n",
    "print(f'Used languages: {langs}')\n",
    "\n",
    "# Group publications by language\n",
    "publications_df = publications_df.groupby('abstract_lang')\n",
    "\n",
    "publications_en = publications_df.get_group('en')\n",
    "publications_ro = publications_df.get_group('ro')\n",
    "\n",
    "publications_en = publications_en.drop(columns=['abstract_lang'])\n",
    "publications_ro = publications_ro.drop(columns=['abstract_lang'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publications_by_user_name(publications, first_name, last_name):\n",
    "    if len(first_name) == 0 or len(last_name) == 0:\n",
    "        print('First name or last name is empty, returning full publications set')\n",
    "        return publications, pd.DataFrame()\n",
    "    first_name = first_name.lower()\n",
    "    last_name = last_name.lower()\n",
    "    user = users_df.loc[(users_df['first_name'].str.lower().str.contains(first_name)) & (users_df['last_name'].str.lower().str.contains(last_name))].values[0]\n",
    "    user_publications_ids = users_validated_df.loc[users_validated_df['user_id'] == user[0]]['publication_id'].values\n",
    "    user_publications = publications.loc[publications['id'].isin(user_publications_ids)]\n",
    "    if len(user_publications) == 0:\n",
    "        print(f'No publications found in this set for user: {user}')\n",
    "        return user_publications, pd.DataFrame()\n",
    "    random_single_publication = user_publications.sample()\n",
    "    user_publications = user_publications.loc[user_publications['id'] != random_single_publication['id'].values[0]]\n",
    "    return user_publications, random_single_publication\n",
    "\n",
    "user_first_name = input('Enter user first name: ')\n",
    "user_last_name = input('Enter user last name: ')\n",
    "publications_en, random_single_publication_en = get_publications_by_user_name(publications_en, user_first_name, user_last_name)\n",
    "publications_ro, random_single_publication_ro = get_publications_by_user_name(publications_ro, user_first_name, user_last_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(texts):\n",
    "    if len(texts) == 0:\n",
    "        return texts\n",
    "    # Remove punctuation, newlines and tabs\n",
    "    texts = texts.map(lambda x: re.sub('[,\\\\.!?`\\'\\n\\t•„\"]', '', x))\n",
    "    # Remove numbers\n",
    "    texts = texts.map(lambda x: re.sub('[0-9]', '', x))\n",
    "    # Convert to lowercase\n",
    "    texts = texts.map(lambda x: x.lower())\n",
    "    return texts\n",
    "\n",
    "publications_en['abstract_text_processed'] = cleanup_text(publications_en['abstract_text'])\n",
    "print(publications_en['abstract_text_processed'].head())\n",
    "publications_ro['abstract_text_processed'] = cleanup_text(publications_ro['abstract_text'])\n",
    "print(publications_ro['abstract_text_processed'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def build_bigram_trigram_models(data_words):\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    return bigram_mod, trigram_mod\n",
    "\n",
    "def remove_stopwords(texts, stop_words):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts, bigram_mod, trigram_mod):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def preprocess_text(data, lemmatization_model, stop_words):\n",
    "    if len(data) == 0 or len(lemmatization_model) == 0 or len(stop_words) == 0:\n",
    "        return []\n",
    "    \n",
    "    # tokenize\n",
    "    data_words = list(split_words(data))\n",
    "\n",
    "    # Initialize spacy model, keeping only tagger component (for efficiency)\n",
    "    nlp = spacy.load(lemmatization_model, disable=['parser', 'ner'])\n",
    "\n",
    "    # lemmatization\n",
    "    data_words = lemmatization(data_words, nlp)\n",
    "\n",
    "    # remove stop words\n",
    "    data_words = remove_stopwords(data_words, stop_words)\n",
    "\n",
    "    bigram_mod, trigram_mod = build_bigram_trigram_models(data_words)\n",
    "\n",
    "    # # form bigrams\n",
    "    # data_words = make_bigrams(data_words, bigram_mod)\n",
    "\n",
    "    return data_words\n",
    "\n",
    "# python3 -m spacy download en_core_web_trf\n",
    "lemmatization_model_en = 'en_core_web_trf'\n",
    "# python3 -m spacy download ro_core_news_lg\n",
    "lemmatization_model_ro = 'ro_core_news_lg'\n",
    "\n",
    "stop_words_en = stopwords.words('english')\n",
    "stop_words_en.extend(['from', 'subject', 're', 'edu', 'use', 'result', 'datum'])\n",
    "stop_words_ro = stopwords.words('romanian')\n",
    "stop_words_ro.extend(['tip', 'of', 'the', 'for'])\n",
    "\n",
    "data_en = publications_en['abstract_text_processed'].values.tolist()\n",
    "data_ro = publications_ro['abstract_text_processed'].values.tolist()\n",
    "\n",
    "data_words_en = preprocess_text(data_en, lemmatization_model_en, stop_words_en)\n",
    "data_words_ro = preprocess_text(data_ro, lemmatization_model_ro, stop_words_ro)\n",
    "\n",
    "print(data_words_en[:1])\n",
    "print(data_words_ro[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_wordcloud(data_words):\n",
    "    if len(data_words) == 0:\n",
    "        data_words = [['none']]\n",
    "    # Join the different processed titles together.\n",
    "    long_string = ','.join([word for sublist in data_words for word in sublist])\n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue', width=800, height=600)\n",
    "    # Generate a word cloud\n",
    "    wordcloud.generate(long_string)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_wordcloud(data_words_en).to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_wordcloud(data_words_ro).to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_and_corpus(data_words):\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    # Create Corpus\n",
    "    texts = data_words\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    return id2word, corpus\n",
    "\n",
    "id2word_en, corpus_en = create_dictionary_and_corpus(data_words_en)\n",
    "id2word_ro, corpus_ro = create_dictionary_and_corpus(data_words_ro)\n",
    "\n",
    "# View\n",
    "print(corpus_en[:1])\n",
    "print(corpus_ro[:1])\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "print([[(id2word_en[id], freq) for id, freq in cp] for cp in corpus_en[:1]])\n",
    "print([[(id2word_ro[id], freq) for id, freq in cp] for cp in corpus_ro[:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set LDA parameters\n",
    "num_topics = 10\n",
    "num_words = 20\n",
    "chunksize = 2000\n",
    "passes = 5\n",
    "alpha = 'asymmetric'\n",
    "eta = 'auto'\n",
    "iterations = 500\n",
    "eval_every = 1\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lda_model(corpus, id2word, num_topics_param, chunksize_param, passes_param, alpha_param, eta_param, eval_every_param, iterations_param, workers_param):\n",
    "    if len(corpus) == 0 or len(id2word) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=num_topics_param,\n",
    "        chunksize=chunksize_param,\n",
    "        passes=passes_param,\n",
    "        alpha=alpha_param,\n",
    "        eta=eta_param,\n",
    "        eval_every=eval_every_param,\n",
    "        iterations=iterations_param,\n",
    "        workers=workers_param\n",
    "    )\n",
    "\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "lda_model_en = build_lda_model(corpus_en, id2word_en, num_topics, chunksize, passes, alpha, eta, eval_every, iterations, workers)\n",
    "lda_model_ro = build_lda_model(corpus_ro, id2word_ro, num_topics, chunksize, passes, alpha, eta, eval_every, iterations, workers)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "if lda_model_en is not None:\n",
    "    pprint(lda_model_en.print_topics(num_topics=num_topics, num_words=num_words))\n",
    "    doc_lda_en = lda_model_en[corpus_en]\n",
    "if lda_model_ro is not None:\n",
    "    pprint(lda_model_ro.print_topics(num_topics=num_topics, num_words=num_words))\n",
    "    doc_lda_ro = lda_model_ro[corpus_ro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(lda_model, model_description, corpus, id2word, data_words):\n",
    "    if lda_model is None or len(corpus) == 0 or len(id2word) == 0 or len(data_words) == 0:\n",
    "        print(f'No metrics for {model_description}\\n')\n",
    "        return\n",
    "    \n",
    "    # Compute Perplexity\n",
    "    print(f'Perplexity for {model_description}: {lda_model.log_perplexity(corpus)}\\n')  # a measure of how good the model is. lower the better.\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f'Coherence Score for {model_description}: {coherence_lda}\\n')\n",
    "\n",
    "compute_metrics(lda_model_en, 'EN', corpus_en, id2word_en, data_words_en)\n",
    "compute_metrics(lda_model_ro, 'RO', corpus_ro, id2word_ro, data_words_ro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = lda_model_en\n",
    "corpus = corpus_en\n",
    "id2word = id2word_en\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_single_publication(random_single_publication, model_to_compare, lemmatization_model, stop_words):\n",
    "    random_single_publication['abstract_text_processed'] = cleanup_text(random_single_publication['abstract_text'])\n",
    "\n",
    "    data = random_single_publication['abstract_text_processed'].values.tolist()\n",
    "    data_words = preprocess_text(data, lemmatization_model, stop_words)\n",
    "\n",
    "    id2word, corpus = create_dictionary_and_corpus(data_words)\n",
    "\n",
    "    lda_model = build_lda_model(corpus, id2word, num_topics, chunksize, passes, alpha, eta, eval_every, iterations, workers)\n",
    "    if lda_model is not None:\n",
    "        # pprint(lda_model.print_topics(num_topics=num_topics, num_words=num_words))\n",
    "        # pprint(model_to_compare.print_topics(num_topics=num_topics, num_words=num_words))\n",
    "        mdiff, annotation = lda_model.diff(model_to_compare, distance='jaccard', num_words=num_words, n_ann_terms=num_words)\n",
    "        # print(mdiff)\n",
    "        # print(annotation)\n",
    "        return mdiff, annotation\n",
    "    else:\n",
    "        print('No model')\n",
    "        return None, None\n",
    "\n",
    "def plot_difference(mdiff, title=\"\"):\n",
    "    _, ax = plt.subplots(figsize=(10, 10))\n",
    "    data = ax.imshow(mdiff, cmap='RdBu_r', origin='lower', vmin=0.75, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(data)\n",
    "\n",
    "\n",
    "mdiff, annotation = compare_single_publication(random_single_publication_en, lda_model_en, lemmatization_model_en, stop_words_en)\n",
    "print(f\"Picked random paper from same author:\\n{random_single_publication_en['title'].values[0]}\\nfrom authors:\\n{random_single_publication_en['authors'].values[0]}\")\n",
    "plot_difference(mdiff, title=\"Topic difference (random paper from same author) [jaccard distance]\")\n",
    "\n",
    "publications_all_en = publications_df.get_group('en')\n",
    "other_random_single_publication_en = publications_all_en.sample()\n",
    "print(f\"Picked random paper from full dataset:\\n{other_random_single_publication_en['title'].values[0]}\\nfrom authors:\\n{other_random_single_publication_en['authors'].values[0]}\")\n",
    "mdiff, annotation = compare_single_publication(other_random_single_publication_en, lda_model_en, lemmatization_model_en, stop_words_en)\n",
    "plot_difference(mdiff, title=\"Topic difference (random paper from dataset) [jaccard distance]\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
